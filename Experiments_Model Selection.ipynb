{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results path\n",
    "import os\n",
    "results_path = 'Results/ModelSelection/'\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions\"\"\"\n",
    "import GCA\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "from numpy import linalg as LA\n",
    "import math as m\n",
    "from numpy.linalg import det,inv,norm\n",
    "from scipy import linalg, stats, sparse\n",
    "from scipy.sparse import tril\n",
    "from scipy.special import multigammaln\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.utils.extmath import fast_logdet\n",
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "from sklearn.covariance import GraphicalLassoCV, GraphicalLasso\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "from cvxopt import solvers, matrix, spmatrix, log, mul, blas, lapack, amd, cholmod\n",
    "import itertools\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "def covsel1(Y , K):\n",
    "    \"\"\"\n",
    "    Returns the solution of\n",
    " \n",
    "        minimize    -log det K + tr(KY)\n",
    "        subject to  K_ij = 0  if (i,j) not in zip(I, J).\n",
    "    Y is a symmetric sparse matrix with nonzero diagonal elements.\n",
    "    I = Y.I,  J = Y.J.\n",
    "    Here we use estimation of precision matrix from the previous step K as initialization point\n",
    "    \"\"\"\n",
    "\n",
    "    cholmod.options['supernodal'] = 2\n",
    "\n",
    "    I, J = Y.I, Y.J\n",
    "    n, m = Y.size[0], len(I) \n",
    "    # non-zero positions for one-argument indexing \n",
    "    N = I + J*n         \n",
    "    # position of diagonal elements\n",
    "    D = [ k for k in range(m) if I[k]==J[k] ]  \n",
    "    \n",
    "    # Kn is used in the line search\n",
    "    Kn = spmatrix(0.0, I, J)\n",
    "\n",
    "    # symbolic factorization of K \n",
    "    F = cholmod.symbolic(K)\n",
    "\n",
    "    # Kinv will be the inverse of K\n",
    "    Kinv = matrix(0.0, (n,n))\n",
    "\n",
    "    for iters in range(100):\n",
    "        # numeric factorization of K\n",
    "        cholmod.numeric(K, F)\n",
    "        d = cholmod.diag(F)\n",
    "\n",
    "        # compute Kinv by solving K*X = I \n",
    "        Kinv[:] = 0.0\n",
    "        Kinv[::n+1] = 1.0\n",
    "        cholmod.solve(F, Kinv)\n",
    "        \n",
    "        # solve Newton system\n",
    "        grad = 2 * (Y.V - Kinv[N])\n",
    "        hess = 2 * ( mul(Kinv[I,J], Kinv[J,I]) + \n",
    "               mul(Kinv[I,I], Kinv[J,J]) )\n",
    "        v = -grad\n",
    "        lapack.posv(hess,v) \n",
    "                                                  \n",
    "        # stopping criterion\n",
    "        sqntdecr = -blas.dot(grad,v) \n",
    "        #print(\"Newton decrement squared:%- 7.5e\" %sqntdecr)\n",
    "        if (sqntdecr < 1e-12):\n",
    "            #print(\"number of iterations: %d\" %(iters+1))\n",
    "            break\n",
    "\n",
    "        # line search\n",
    "        dx = +v\n",
    "        dx[D] *= 2      \n",
    "        f = -2.0*sum(log(d))      # f = -log det K\n",
    "        s = 1\n",
    "        for lsiter in range(50):\n",
    "            Kn.V = K.V + s*dx\n",
    "            try: \n",
    "                cholmod.numeric(Kn, F)\n",
    "            except ArithmeticError: \n",
    "                s *= 0.5\n",
    "            else:\n",
    "                d = cholmod.diag(F)\n",
    "                fn = -2.0 * sum(log(d)) + 2*s*blas.dot(v,Y.V)\n",
    "                if (fn < f - 0.01*s*sqntdecr): break\n",
    "                else: s *= 0.5\n",
    "\n",
    "        K.V = Kn.V\n",
    "    C = np.zeros([K.size[0],K.size[0]])\n",
    "    for i,j,v in zip(K.I,K.J,K.V):\n",
    "        C[i,j] = v\n",
    "        C[j,i] = v\n",
    "    return inv(C)\n",
    "\n",
    "def scipy_sparse_to_spmatrix(A):\n",
    "    coo = A.tocoo()\n",
    "    SP = spmatrix(coo.data.tolist(), coo.row.tolist(), coo.col.tolist(), size=A.shape)\n",
    "    return SP\n",
    "\n",
    "def Codelength_UniversalCoder(n,GG):\n",
    "    cl= np.zeros(8)\n",
    "    GG.remove_edges_from(nx.selfloop_edges(GG))\n",
    "    pg = GCA.CS12s()\n",
    "    pg.parse(n, GG, update=True)\n",
    "    cons = n - 0.5 * np.log2(n)\n",
    "    cl[0] = pg.codelength_bin() \n",
    "    cl[1] = pg.codelength_degree() + cons \n",
    "    cl[2] = pg.codelength_class1(ctype='tri') \n",
    "    cl[3] = pg.codelength_class2(ctype='tri') + cons \n",
    "    cl[4] = pg.codelength_class1(ctype='com') \n",
    "    cl[5] = pg.codelength_class2(ctype='com') + cons \n",
    "    cl[6] = pg.codelength_class1(ctype='4nod') \n",
    "    cl[7] = pg.codelength_class2(ctype='4nod')+ cons\n",
    "    return cl*m.log(2)\n",
    "\n",
    "def Codelength_data2(n,GG,X, B=10):\n",
    "    \"\"\"calculate codelength of data based on estimated covaraince. Here we estimated covariance\n",
    "    from each step to initilize the next step A2. Also we encode using block coding with B blocks\n",
    "    \"\"\"\n",
    "    sigma = []\n",
    "    A = nx.to_scipy_sparse_matrix(GG)\n",
    "    A = tril(A)\n",
    "    A_hat= scipy_sparse_to_spmatrix(A)\n",
    "    A_adj = nx.adjacency_matrix(GG).todense()\n",
    "    cl_est = 0\n",
    "    start = int(0.25*X.shape[1]) # where to start coding with estimated covariance matrix\n",
    "    # default distribution for first few samples\n",
    "    for j in range(start):\n",
    "        cl_est += 0.5*n*m.log(2*m.pi) + 0.5*np.dot(X[j,:],X[j,:])\n",
    "    \n",
    "    # estimated covariance matrix for the rest of samples\n",
    "    A2 = spmatrix(0.0, A_hat.I , A_hat.J) # starting point: symmetric identity with nonzero pattern I,J\n",
    "    A2[::n+1] = 1.0\n",
    "    prec_est = np.ones([n, n])\n",
    "    M = np.ceil((X.shape[0]-start)/B).astype(int)\n",
    "    for j in range(B):\n",
    "        s1 = min(start+j*M, X.shape[0])\n",
    "        s2 = min(X.shape[0]-s1, M)\n",
    "        sigma_emp = np.matmul(X[:s1,:].transpose(),X[:s1,:])/s1\n",
    "        A1 = spmatrix([sigma_emp[r,p] for r,p in zip(A_hat.I,A_hat.J)], A_hat.I , A_hat.J)\n",
    "        sigma_est = covsel1(A1, A2)\n",
    "        prec_est = inv(sigma_est)\n",
    "        A2 = spmatrix([prec_est[r,p] for r,p in zip(A_hat.I,A_hat.J)], A_hat.I , A_hat.J)\n",
    "        cl_est += s2*(0.5*m.log(det(sigma_est)) + 0.5*n*m.log(2*m.pi))\n",
    "        for k in range(s2):\n",
    "            cl_est += 0.5*np.dot(X[s1+k,:]@inv(sigma_est),X[s1+k,:])\n",
    "    return cl_est\n",
    "\n",
    "\n",
    "def Codelength_AllStructures2(X_, prec_mats):\n",
    "    \"\"\"By initializing covariance matrix for Dempster method and also block coding\"\"\"\n",
    "    CL_data = []\n",
    "    CL_graph = []\n",
    "    for j in range(len(prec_mats)):\n",
    "        G_ = nx.Graph(prec_mats[j])\n",
    "        temp = Codelength_UniversalCoder(prec_mats[j].shape[0],G_)\n",
    "        CL_graph.append(temp)\n",
    "        \n",
    "        G_ = nx.Graph(prec_mats[j])\n",
    "        temp2 = Codelength_data2(prec_mats[j].shape[0],G_, X_, B=10)\n",
    "        CL_data.append(temp2)\n",
    "    \n",
    "    return CL_data, CL_graph\n",
    "\n",
    "def prec_generation (graph_type, dim, alpha=0.02):\n",
    "    # graph_type: circular, AR4, ER\n",
    "    # alpha: prob. of having edge in ER graph\n",
    "    prec = np.zeros([dim,dim])\n",
    "    if graph_type == 'cycle':\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                if abs(i-j) == 1: prec[i,j] = 0.45\n",
    "        prec[0,dim-1]= prec[dim-1,0]=0.45\n",
    "        prec += np.eye(dim)\n",
    "    \n",
    "    if graph_type == 'AR1':\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                if abs(i-j) == 1: prec[i,j] = 0.45\n",
    "        prec += np.eye(dim)\n",
    "        \n",
    "    if graph_type == 'ER':\n",
    "        for i in range(dim-1):\n",
    "            for j in range(i+1,dim):\n",
    "                if random.random() <= alpha: prec[i,j] = random.uniform(0.4,0.8)\n",
    "        prec += prec.transpose()\n",
    "        w, v = np.linalg.eig(prec)\n",
    "        prec += (abs(min(w)) + 0.05)*np.eye(dim)\n",
    "        d = np.sqrt(np.diag(prec))\n",
    "        prec /= d \n",
    "        prec /= d[:, np.newaxis]\n",
    "    \n",
    "    if graph_type == 'Hub':\n",
    "        tr = 0.01\n",
    "        n_hubs = 2\n",
    "        for i in range(dim-1):\n",
    "            for j in range(i+1,dim):\n",
    "                if random.random() <= tr:\n",
    "                    prec[i,j] = 1\n",
    "        hubs = random.sample(range(dim), n_hubs)\n",
    "        for i in hubs:\n",
    "            for j in range(dim):\n",
    "                if i!=j:\n",
    "                    if random.random() <= 0.7: prec[i,j] = 1\n",
    "                    else: prec[i,j] = 0\n",
    "        for i in range(dim-1):\n",
    "            for j in range(i+1,dim):\n",
    "                if i!=j and prec[i,j]==1:\n",
    "                    if (random.random() <= 0.5): prec[i,j] = random.uniform(-0.75,-0.25)\n",
    "                    else: prec[i,j] = random.uniform(0.25,0.75)\n",
    "\n",
    "        prec = 0.5*(prec+prec.transpose())\n",
    "        w, v = np.linalg.eig(prec)\n",
    "        prec += (0.1 + abs(min(w)))*np.eye(dim)\n",
    "        d = np.sqrt(np.diag(prec))\n",
    "        prec /= d\n",
    "        prec /= d[:, np.newaxis]\n",
    "    \n",
    "    if graph_type == 'BA':\n",
    "        m = 2 # Number of edges to attach from a new node\n",
    "        p = 0.5 # probability of adding a triangle after adding a random edge in power-law cluster model\n",
    "        G = nx.barabasi_albert_graph(dim, m)\n",
    "        #G = nx.powerlaw_cluster_graph(dim, m, p)\n",
    "        prec_temp = nx.to_numpy_array(G).astype('int')\n",
    "        for i in range(dim-1):\n",
    "            for j in range(i+1,dim):\n",
    "                if i!=j and prec_temp[i,j]==1:\n",
    "                    if (random.random() <= 0.5): prec[i,j] = random.uniform(-0.75,-0.25)\n",
    "                    else: prec[i,j] = random.uniform(0.25,0.75)\n",
    "\n",
    "        prec = 0.5*(prec+prec.transpose())\n",
    "        w, v = np.linalg.eig(prec)\n",
    "        prec += (0.1 + abs(min(w)))*np.eye(dim)\n",
    "        d = np.sqrt(np.diag(prec))\n",
    "        prec /= d\n",
    "        prec /= d[:, np.newaxis]\n",
    "    \n",
    "    if graph_type == 'WS':\n",
    "        pr = 0.25  # probability of rewiring each edge\n",
    "        k = 4 # Each node is joined with its k nearest neighbors\n",
    "        G = nx.watts_strogatz_graph(dim, k, pr)\n",
    "        prec_temp = nx.to_numpy_array(G).astype('int')\n",
    "        for i in range(dim-1):\n",
    "            for j in range(i+1,dim):\n",
    "                if i!=j and prec_temp[i,j]==1:\n",
    "                    if (random.random() <= 0.5): prec[i,j] = random.uniform(-0.75,-0.25)\n",
    "                    else: prec[i,j] = random.uniform(0.25,0.75)\n",
    "\n",
    "        prec = 0.5*(prec+prec.transpose())\n",
    "        w, v = np.linalg.eig(prec)\n",
    "        prec += (0.1 + abs(min(w)))*np.eye(dim)\n",
    "        d = np.sqrt(np.diag(prec))\n",
    "        prec /= d\n",
    "        prec /= d[:, np.newaxis]\n",
    "    \n",
    "    return prec\n",
    "\n",
    "def generate_sample(prec, n_samp, n_feat):\n",
    "    prng = np.random.RandomState()\n",
    "    cov = linalg.inv(prec)\n",
    "    d = np.sqrt(np.diag(cov))\n",
    "    cov /= d\n",
    "    cov /= d[:, np.newaxis]\n",
    "    x = prng.multivariate_normal(np.zeros(n_feat), cov, size=n_samp)\n",
    "    x -= x.mean(axis=0)\n",
    "    x /= x.std(axis=0)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions for L1 solvers and model selection techniques\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.covariance import log_likelihood\n",
    "from sklearn.utils.extmath import fast_logdet\n",
    "\n",
    "try:\n",
    "    import rpy2\n",
    "    from rpy2.robjects.packages import importr\n",
    "    import rpy2.robjects.numpy2ri as swap\n",
    "\n",
    "    R = rpy2.robjects.r\n",
    "    importr('glasso')\n",
    "    importr('BigQuic')\n",
    "    importr('dpglasso')\n",
    "except:\n",
    "    msg = \"Either R, rpy2, or BigQuic are not installed\"\n",
    "    raise ImportError(msg)\n",
    "\n",
    "\n",
    "def glasso(data, alpha=1.):\n",
    "    \"\"\"Python front-end for GLASSO solver.\"\"\"\n",
    "    data = np.copy(data)\n",
    "    cov_emp = np.dot(data.T, data) / data.shape[0]\n",
    "    S = swap.numpy2rpy(cov_emp)  # Load data into R space\n",
    "    # Construct and run the program in R\n",
    "    program_string = 'f <- function(r) {out = glasso(r, rho=%a); ' % alpha\n",
    "    program_string += 'as(out$wi, \"matrix\")}'\n",
    "    f = R(program_string)\n",
    "    prec = np.array(f(S))\n",
    "    return prec\n",
    "\n",
    "def bigquic(data, alpha=1.):\n",
    "    \"\"\"Python front-end for BigQuic solver.\"\"\"\n",
    "    data = np.copy(data)\n",
    "    rdata = swap.numpy2rpy(data)  # Load data into R space\n",
    "\n",
    "    # Construct and run the program in R\n",
    "    program_string = 'f <- function(r) {out = BigQuic(X=r, lambda=%a, use_ram=TRUE); ' % alpha\n",
    "    program_string += 'as(out$precision_matrices[[1]], \"matrix\")}'\n",
    "    f = R(program_string)\n",
    "    prec = np.array(f(rdata))\n",
    "    return prec\n",
    "\n",
    "def dpglasso(data, alpha=1.):\n",
    "    \"\"\"Python front-end for dpglasso solver.\"\"\"\n",
    "    data = np.copy(data)\n",
    "    cov_emp = np.dot(data.T, data) / data.shape[0]\n",
    "    S = swap.numpy2rpy(cov_emp)  # Load data into R space\n",
    "    # Construct and run the program in R\n",
    "    #program_string = 'f <- function(r) {out = dpglasso(r, rho=%a); ' % alpha\n",
    "    program_string = 'f <- function(r) {out = dpglasso(r, rho=%a, outer.tol=%a); ' % (alpha, 10e-3) # set tolerance\n",
    "    program_string += 'as(out$X, \"matrix\")}'\n",
    "    f = R(program_string)\n",
    "    prec = np.array(f(S))\n",
    "    return prec\n",
    "\n",
    "def find_range(emp_cov):\n",
    "    \"\"\"Find range for lambda\"\"\"\n",
    "    A = np.copy(emp_cov)\n",
    "    A.flat[::A.shape[0]+1] = 0\n",
    "    alpha_1 = np.max(np.abs(A))\n",
    "    alpha_0 = 1e-1 * alpha_1\n",
    "    n_alphas = 50\n",
    "    range_ = np.logspace(np.log10(alpha_0), np.log10(alpha_1) , n_alphas)\n",
    "    return range_\n",
    "\n",
    "def solver_path(data, lamb, solver):\n",
    "    \"\"\" Solve L1 log-likelihood for all lambdas and return precision matrices \"\"\"\n",
    "    num_nodes = data.shape[1]\n",
    "    num_sample = data.shape[0]\n",
    "    prec_matrix = [np.zeros([num_nodes, num_nodes]) for j in range(len(lamb))]\n",
    "    \n",
    "    for j in range(len(lamb)):\n",
    "        if solver == 'glasso': prec_ = glasso(data, lamb[j])\n",
    "        elif solver == 'dpglasso': prec_ = dpglasso(data, lamb[j])\n",
    "        elif solver == 'bigquic': prec_ = bigquic(data, lamb[j])\n",
    "        prec_matrix[j] += prec_\n",
    "    return prec_matrix\n",
    "\n",
    "def solver_single(data, lamb, solver):\n",
    "    \"\"\" Solve L1 log-likelihood for single lambda and return precision matrix \"\"\"\n",
    "    if solver == 'glasso': prec_ = glasso(data, lamb)\n",
    "    elif solver == 'dpglasso': prec_ = dpglasso(data, lamb)\n",
    "    elif solver == 'bigquic': prec_ = bigquic(data, lamb)\n",
    "    return prec_\n",
    "\n",
    "def stars_metric(data, lamb, solver, beta=0.05, N=20):\n",
    "    \"\"\"\n",
    "    Implementation of the StARS (Stability Approach to Regularization Selection) algorithm [Liu et al., 2010]\n",
    "       \n",
    "    lamb: vector of lambda by increasing values (for example lamb = np.logspace(-1,0,10))\n",
    "    data: data\n",
    "    beta: cut point value\n",
    "    N: number of bootstraps\n",
    "    \"\"\"\n",
    "    num_nodes = data.shape[1]\n",
    "    num_sample = data.shape[0]\n",
    "    if num_sample > 144:\n",
    "        num_subsample = int(10*np.sqrt(num_sample))\n",
    "    else:\n",
    "        num_subsample = int(0.8*num_sample)\n",
    "    Adj_matrix = [np.zeros([num_nodes, num_nodes]) for j in range(len(lamb))]\n",
    "    \n",
    "    for ind_boot in range(N):\n",
    "        # we consider 10*sqrt(num_sample) as the size of subsample set\n",
    "        boot_index = np.random.choice(num_sample, num_subsample, replace=False) # Random subsample\n",
    "        # solve sover for each value of lambda\n",
    "        for j in range(len(lamb)):\n",
    "            if solver == 'glasso': prec_ = glasso(data[boot_index, :], lamb[j])\n",
    "            elif solver == 'dpglasso': prec_ = dpglasso(data[boot_index, :], lamb[j])\n",
    "            elif solver == 'bigquic': prec_ = bigquic(data[boot_index, :], lamb[j])\n",
    "            Adj_matrix[j] += 1*(prec_ != 0) # for each edge, add 1 if it is present in the network\n",
    "\n",
    "    # Compute the total instability for each lambda\n",
    "    v = np.zeros(len(lamb))\n",
    "    iu = np.triu_indices(num_nodes, 1)\n",
    "    for j in range(len(lamb)):\n",
    "        Adj_matrix[j] = Adj_matrix[j] / N\n",
    "        D = 2*Adj_matrix[j]*(1-Adj_matrix[j]) # instability of the edges across subsamples\n",
    "        v[j] = np.mean(D[iu]) # total instability\n",
    "    \n",
    "    # Determine the optimal lambda\n",
    "    vcum = [np.max(v[rho:]) for rho in range(len(lamb))]\n",
    "    lambda_opt = np.min(lamb[np.array(vcum) <= beta])\n",
    "    # solve the estimator with lambda_opt and return the precision matrix\n",
    "    return lambda_opt\n",
    "\n",
    "def cv_metric(x, alphas, solver, k=5):\n",
    "    \"\"\"Choose sparsity hyper-parameter using k-fold cross validation,\n",
    "    The documentation recommends NOT to use alpha < 0.4 for performance reasons...\"\"\"\n",
    "    kf = KFold(n_splits=k)\n",
    "    cv_dict = {}\n",
    "    for alpha in alphas:\n",
    "        for x_train, x_test in kf.split(x):\n",
    "            if solver == 'glasso': prec_ = glasso(x[x_train], alpha)\n",
    "            elif solver == 'dpglasso': prec_ = dpglasso(x[x_train], alpha)\n",
    "            elif solver == 'bigquic': prec_ = bigquic(x[x_train], alpha)\n",
    "            emp_cov = np.dot(x[x_test].T, x[x_test]) / x[x_test].shape[0]\n",
    "            score = log_likelihood(emp_cov, prec_)\n",
    "            cv_dict[alpha] = cv_dict.get(alpha, []) + [score]\n",
    "    best_alpha = sorted([(-np.mean(v), k) for k, v in cv_dict.items()])[0][1]\n",
    "    return best_alpha\n",
    "\n",
    "def ebic_metric(data, prec_mats, gamma=0):\n",
    "    \"\"\"\n",
    "    Extended Bayesian Information Criteria for model selection.\n",
    "    data : 2D ndarray (n_samples, n_features)\n",
    "    precision : 2D ndarray (n_features, n_features)\n",
    "        The precision matrix of the model to be tested\n",
    "    gamma : (float) \\in (0, 1)\n",
    "        Choice of gamma=0 leads to classical BIC\n",
    "        Positive gamma leads to stronger penalization of large graphs.\n",
    "    Returns\n",
    "    -------\n",
    "    ebic score (float).  Caller should minimized this score.\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    n_features = data.shape[1]\n",
    "    covariance = np.dot(data.T, data) / n_samples\n",
    "    score = np.zeros(len(prec_mats))\n",
    "    for i in range(len(prec_mats)):\n",
    "        l_theta = 0\n",
    "        l_theta = -np.sum(covariance * prec_mats[i]) + fast_logdet(prec_mats[i])\n",
    "        l_theta *= n_features / 2.\n",
    "\n",
    "        # if something goes wrong with fast_logdet, return large value\n",
    "        if np.isinf(l_theta) or np.isnan(l_theta):\n",
    "            l_theta = 1e10\n",
    "        mask = np.abs(prec_mats[i].flat) > np.finfo(prec_mats[i].dtype).eps\n",
    "        precision_nnz = (np.sum(mask) - n_features) / 2.0  # lower off diagonal tri\n",
    "        score[i] = -2.0 * l_theta + precision_nnz * np.log(n_samples) + 4.0 * precision_nnz * np.log(n_features) * gamma\n",
    "    \n",
    "    idx = np.argmin(score)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Select parameteres\"\"\"\n",
    "size = 100 # size of graph\n",
    "iters = 50 # number of trials\n",
    "n_features = size # number of variables\n",
    "n_samples = 2*size # number of samples\n",
    "# select graph strcuture\n",
    "graph_type = ['cycle', 'AR1', 'ER', 'Hub', 'BA', 'WS']\n",
    "graph = graph_type[-2]\n",
    "# select solver\n",
    "solvers = ['glasso', 'dpglasso', 'bigquic']\n",
    "solver = solvers[1] # picke the solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main function\"\"\"\n",
    "data = []\n",
    "precision = []\n",
    "t0 = time.time()\n",
    "##########################################################33\n",
    "# data generation\n",
    "for t in range(iters):      \n",
    "    prng = np.random.RandomState(t)\n",
    "    prec = prec_generation (graph, n_features)\n",
    "    #prec = prec_generation (graph, n_features, 2/n_features)\n",
    "    cov = linalg.inv(prec)\n",
    "    d = np.sqrt(np.diag(cov))\n",
    "    cov /= d\n",
    "    cov /= d[:, np.newaxis]\n",
    "    prec *= d\n",
    "    prec *= d[:, np.newaxis]\n",
    "    X = prng.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    X -= X.mean(axis=0)\n",
    "    X /= X.std(axis=0)\n",
    "    data.append(X)\n",
    "    precision.append(prec)\n",
    "##############################################################################\n",
    "# estimate covariance matrix\n",
    "EBIC = []\n",
    "BIC = []\n",
    "CV = []\n",
    "STARS = []\n",
    "CODING = []\n",
    "for t in range(iters):\n",
    "    if(t%5 == 0): print(\"iter:\", t)\n",
    "    X = np.copy(data[t])\n",
    "    emp_cov = np.dot(X.T, X) / n_samples\n",
    "    lambdaRange = find_range(emp_cov) # for n_samples > n_features\n",
    "    #lambdaRange = np.logspace(-1, 0, 50) # for n_samples < n_features\n",
    "    prec_path = solver_path(X, lambdaRange, solver)\n",
    "    \n",
    "    lambda_stars = stars_metric(X, lambdaRange, solver)\n",
    "    prec_stars = solver_single(X, lambda_stars, solver)\n",
    "    STARS.append(prec_stars)\n",
    "    \n",
    "    lambda_cv = cv_metric(X, lambdaRange, solver)\n",
    "    prec_cv = solver_single(X, lambda_cv, solver)\n",
    "    CV.append(prec_cv)\n",
    "    \n",
    "    bias = 15\n",
    "    cl_d, cl_g = Codelength_AllStructures2(X, prec_path[bias:]) # coding wih one precision matrix\n",
    "    #cl_d, cl_g = Codelength_AllStructures3(X, prec_path[bias:-10], 100) # coding with precision submatrices size as input\n",
    "    coding_prec = []\n",
    "    for d in range(8):\n",
    "        coding_idx = np.argmin([i+j for i,j in zip(cl_d, [cl[d] for cl in cl_g])])\n",
    "        coding_prec.append(prec_path[coding_idx+bias])\n",
    "    CODING.append(coding_prec)\n",
    "    \n",
    "    ebic_idx = ebic_metric(X, prec_path, 0.5)\n",
    "    EBIC.append(prec_path[ebic_idx])\n",
    "    \n",
    "    bic_idx = ebic_metric(X, prec_path)\n",
    "    BIC.append(prec_path[bic_idx])\n",
    "    \n",
    "print(\"total time: \",time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"save input data\"\"\"\n",
    "f = [data, precision, EBIC, BIC, CV, STARS, CODING]\n",
    "with open(results_path+ graph + '_'+solver +'_feature_%s_sample_%s.text'%(n_features, n_samples),'wb') as fp:\n",
    "    pickle.dump(f, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_metrics(true, est):\n",
    "    metrics = []\n",
    "    for i in range(len(true)):\n",
    "        bool_true = true[i] != 0\n",
    "        bool_est = est[i] != 0\n",
    "        common_edge = np.sum(bool_true & bool_est)\n",
    "        diag = bool_true.shape[0]\n",
    "        precision = round((common_edge - diag) / (np.sum(bool_est) - diag),3)\n",
    "        recall = round((common_edge - diag) / (np.sum(bool_true) - diag),3)\n",
    "        F1 = round(2*precision*recall /(precision + recall),3)    \n",
    "        metrics.append([precision, recall, F1])\n",
    "    return metrics\n",
    "\n",
    "CV_metrics = perf_metrics(precision, CV)\n",
    "BIC_metrics = perf_metrics(precision, BIC)\n",
    "EBIC_metrics = perf_metrics(precision, EBIC)\n",
    "STARS_metrics = perf_metrics(precision, STARS)\n",
    "CODING_metrics = []\n",
    "for i in range(8):\n",
    "    mats = [mat[i] for mat in CODING]\n",
    "    CODING_metrics.append(perf_metrics(precision, mats))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean results for dpglasso, BA\n",
      "cv: [0.18 0.82 0.3 ]\n",
      "stars: [0.52 0.56 0.54]\n",
      "bic: [0.76 0.32 0.44]\n",
      "ebic: [0.94 0.09 0.17]\n",
      "graph coding:\n",
      "iid: [0.77 0.34 0.46]\n",
      "class1_tri: [0.75 0.35 0.47]\n",
      "class1_com: [0.74 0.36 0.48]\n",
      "class1_4nod: [0.75 0.35 0.48]\n",
      "class2_degree: [0.77 0.34 0.46]\n",
      "class2_tri: [0.75 0.35 0.47]\n",
      "class2_com: [0.75 0.35 0.47]\n",
      "class2_4nod: [0.75 0.35 0.47]\n"
     ]
    }
   ],
   "source": [
    "print('mean results for', solver + ', ' + graph)\n",
    "print('cv:', np.round(np.nanmean(CV_metrics, 0),2))\n",
    "print('stars:', np.round(np.nanmean(STARS_metrics, 0),2))\n",
    "print('bic:', np.round(np.nanmean(BIC_metrics, 0),2))\n",
    "print('ebic:', np.round(np.nanmean(EBIC_metrics, 0),2))\n",
    "print('graph coding:')\n",
    "print('iid:', np.round(np.nanmean(CODING_metrics[0], 0), 2))\n",
    "print('class1_tri:', np.round(np.nanmean(CODING_metrics[2], 0), 2))\n",
    "print('class1_com:', np.round(np.nanmean(CODING_metrics[4], 0), 2)) \n",
    "print('class1_4nod:', np.round(np.nanmean(CODING_metrics[6], 0), 2))\n",
    "print('class2_degree:', np.round(np.nanmean(CODING_metrics[1], 0), 2))\n",
    "print('class2_tri:', np.round(np.nanmean(CODING_metrics[3], 0), 2))\n",
    "print('class2_com:', np.round(np.nanmean(CODING_metrics[5], 0), 2))\n",
    "print('class2_4nod:', np.round(np.nanmean(CODING_metrics[7], 0), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std results for dpglasso, BA\n",
      "cv: [0.01 0.05 0.02]\n",
      "stars: [0.04 0.07 0.05]\n",
      "bic: [0.09 0.09 0.09]\n",
      "ebic: [0.06 0.04 0.07]\n",
      "graph coding:\n",
      "iid: [0.08 0.08 0.09]\n",
      "class1_tri: [0.08 0.07 0.08]\n",
      "class1_com: [0.08 0.08 0.08]\n",
      "class1_4nod: [0.08 0.08 0.08]\n",
      "class2_degree: [0.08 0.08 0.09]\n",
      "class2_tri: [0.08 0.08 0.08]\n",
      "class2_com: [0.08 0.08 0.08]\n",
      "class2_4nod: [0.08 0.08 0.08]\n"
     ]
    }
   ],
   "source": [
    "print('std results for', solver + ', ' + graph)\n",
    "print('cv:', np.round(np.nanstd(CV_metrics, 0), 2))\n",
    "print('stars:', np.round(np.nanstd(STARS_metrics, 0), 2))\n",
    "print('bic:', np.round(np.nanstd(BIC_metrics, 0), 2))\n",
    "print('ebic:', np.round(np.nanstd(EBIC_metrics, 0), 2))\n",
    "print('graph coding:')\n",
    "print('iid:', np.round(np.nanstd(CODING_metrics[0], 0), 2))\n",
    "print('class1_tri:', np.round(np.nanstd(CODING_metrics[2], 0), 2))\n",
    "print('class1_com:', np.round(np.nanstd(CODING_metrics[4], 0), 2))\n",
    "print('class1_4nod:', np.round(np.nanstd(CODING_metrics[6], 0), 2))\n",
    "print('class2_degree:', np.round(np.nanstd(CODING_metrics[1], 0), 2))\n",
    "print('class2_tri:', np.round(np.nanstd(CODING_metrics[3], 0), 2))\n",
    "print('class2_com:', np.round(np.nanstd(CODING_metrics[5], 0), 2))\n",
    "print('class2_4nod:', np.round(np.nanstd(CODING_metrics[7], 0), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
